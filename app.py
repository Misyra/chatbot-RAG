"""
æ™ºèƒ½é™¶ç“·é—®ç­”ç³»ç»Ÿ - åŸºäºRAGçš„Streamlit Webåº”ç”¨
"""

import os
import sys
import streamlit as st
import traceback
from dotenv import load_dotenv
import logging
import threading
import time
from utils.file_handlers.file_manager import FileManager
from utils.graph.knowledge_graph import KnowledgeGraph
from utils.retrieval.vector_store import VectorStore
from utils.retrieval.retriever import HybridRetriever
from utils.cache.cache_manager import CacheManager
import requests
import json
from typing import Dict, List, Optional
import bcrypt
from datetime import datetime
from utils.helper_functions import read_data_directory  # å¯¼å…¥è¾…åŠ©å‡½æ•°
import torch
from openai import OpenAI


# ä¿®å¤torchç±»æ‰¾ä¸åˆ°çš„é”™è¯¯
torch.classes.__path__ = [
    os.path.join(torch.__path__[0], torch.classes.__file__)
]  


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("é™¶ç“·é—®ç­”ç³»ç»Ÿ")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹é…ç½®
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "deepseek-r1:7b")
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "BGE-M3:latest")
DEBUG_MODE = os.getenv("DEBUG_MODE", "").lower() in ["true", "1", "yes", "y"]
LOG_FILE = os.getenv("LOG_FILE", "")

# é…ç½®æ—¥å¿—æ–‡ä»¶
if LOG_FILE:
    try:
        log_dir = os.path.dirname(LOG_FILE)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
            logger.info(f"å·²åˆ›å»ºæ—¥å¿—ç›®å½•: {log_dir}")
        
        test_file = os.path.join(log_dir, "permission_test.log")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write("æƒé™æµ‹è¯•\n")
        os.remove(test_file)

        file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        logger.addHandler(file_handler)
        os.environ["VECTOR_STORE_LOG_FILE"] = LOG_FILE
    except Exception as e:
        logger.error(f"æ—¥å¿—æ–‡ä»¶é…ç½®å¤±è´¥: {str(e)}")
        logger.error(f"è¯·æ£€æŸ¥æ—¥å¿—ç›®å½• {log_dir} çš„å†™å…¥æƒé™")

# è®¾ç½®è°ƒè¯•æ¨¡å¼
if DEBUG_MODE:
    logger.setLevel(logging.DEBUG)
    os.environ["VECTOR_STORE_DEBUG"] = "true"
    logger.debug("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

# æ‰“å°å…³é”®é…ç½®ä¿¡æ¯
logger.info("ç³»ç»Ÿé…ç½®: OLLAMA_API_URL=%s, OLLAMA_MODEL=%s, EMBEDDINGS_MODEL=%s, DEBUG_MODE=%s, LOG_FILE=%s",
            OLLAMA_API_URL, OLLAMA_MODEL, EMBEDDINGS_MODEL, DEBUG_MODE, LOG_FILE)

# åˆ›å»ºä¸“ç”¨çš„promptæ—¥å¿—è®°å½•å™¨
prompt_logger = logging.getLogger("prompt_logger")
prompt_logger.setLevel(logging.INFO)
if LOG_FILE:
    prompt_handler = logging.FileHandler(f"{LOG_FILE}.prompt", encoding="utf-8")
    prompt_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    prompt_logger.addHandler(prompt_handler)


# é…ç½®é¡µé¢
st.set_page_config(page_title="æ™ºèƒ½é™¶ç“·é—®ç­”ç³»ç»Ÿ", page_icon="ğŸº", layout="wide")

# åˆå§‹åŒ–ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(BASE_DIR, "cache")
DATA_DIR = os.path.join(BASE_DIR, "data")
CUSTOM_FILES_DIR = os.path.join(DATA_DIR, "custom_files")  # ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ç›®å½•
DEFAULT_DATA_DIR = os.path.join(DATA_DIR, "default_data")  # ç³»ç»Ÿé»˜è®¤æ•°æ®ç›®å½•
STORAGE_DIR = os.path.join(BASE_DIR, "storage")
QA_CACHE_DIR = os.path.join(CACHE_DIR, "qa_cache")

# å®šä¹‰æ–‡ä»¶è·¯å¾„
GRAPH_PATH = os.path.join(STORAGE_DIR, "knowledge_graph.gpickle")
VECTOR_STORE_FILENAME = "vector_store.faiss"  # FAISSä¼šè‡ªåŠ¨å¤„ç†ç›®å½•

# ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å’Œ__init__.pyæ–‡ä»¶å­˜åœ¨
def ensure_directories_and_init_files():
    """ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å’Œ__init__.pyæ–‡ä»¶å­˜åœ¨"""
    # åˆ›å»ºå¿…éœ€çš„ç›®å½•
    for directory in [
        CACHE_DIR,
        DATA_DIR,
        CUSTOM_FILES_DIR,
        DEFAULT_DATA_DIR,
        STORAGE_DIR,
        QA_CACHE_DIR,
    ]:
        os.makedirs(directory, exist_ok=True)

    # ç¡®ä¿utilsç›®å½•ç»“æ„å®Œæ•´
    utils_dirs = [
        os.path.join(BASE_DIR, "utils"),
        os.path.join(BASE_DIR, "utils", "file_handlers"),
        os.path.join(BASE_DIR, "utils", "graph"),
        os.path.join(BASE_DIR, "utils", "retrieval"),
        os.path.join(BASE_DIR, "utils", "cache"),
    ]

    # åˆ›å»ºç›®å½•å’Œåˆå§‹åŒ–æ–‡ä»¶
    for directory in utils_dirs:
        os.makedirs(directory, exist_ok=True)
        init_file = os.path.join(directory, "__init__.py")
        if not os.path.exists(init_file):
            with open(init_file, "w", encoding="utf-8") as f:
                f.write("# è‡ªåŠ¨ç”Ÿæˆçš„åˆå§‹åŒ–æ–‡ä»¶\n")


# æ‰§è¡Œç›®å½•å’Œæ–‡ä»¶æ£€æŸ¥
ensure_directories_and_init_files()

# --- ç»„ä»¶åˆå§‹åŒ–å‡½æ•° ---
def init_components():
    """å»¶è¿Ÿåˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
    try:
        from utils.file_handlers.file_manager import FileManager
        from utils.retrieval.vector_store import VectorStore
        from utils.retrieval.retriever import HybridRetriever
        from utils.graph.knowledge_graph import KnowledgeGraph
        from utils.cache.cache_manager import CacheManager

        # åˆå§‹åŒ–ç»„ä»¶
        logger.info("åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")

        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        file_manager = FileManager(CACHE_DIR, DATA_DIR)

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = CacheManager(QA_CACHE_DIR)

        # åˆå§‹åŒ–å›¾è°±ç®¡ç†å™¨
        knowledge_graph = KnowledgeGraph()

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store = VectorStore(
            STORAGE_DIR,
            embedding_model=EMBEDDINGS_MODEL,
            base_url=OLLAMA_API_URL,
            debug_mode=DEBUG_MODE,
            log_file=LOG_FILE,
        )

        # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
        try:
            logger.info(f"å°è¯•åŠ è½½å‘é‡ç´¢å¼•: {VECTOR_STORE_FILENAME}")
            vector_store.load(VECTOR_STORE_FILENAME)
            # ä¸è¦åœ¨åå°çº¿ç¨‹ä¸­è°ƒç”¨st.sidebarç­‰UIç»„ä»¶
            logger.info("å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ")
        except Exception as e:
            error_msg = f"æœªæ‰¾åˆ°æˆ–åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}"
            logger.warning(error_msg)
            if DEBUG_MODE:
                logger.debug(f"åŠ è½½å‘é‡ç´¢å¼•è¯¦ç»†é”™è¯¯ä¿¡æ¯: {str(e)}")

        # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        hybrid_retriever = HybridRetriever(vector_store)

        # å°è¯•åŠ è½½çŸ¥è¯†å›¾è°±
        try:
            if os.path.exists(GRAPH_PATH):
                logger.info(f"å°è¯•åŠ è½½çŸ¥è¯†å›¾è°±: {GRAPH_PATH}")
                knowledge_graph.load_graph(GRAPH_PATH)
                logger.info("çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ")
            else:
                logger.info("æœªæ‰¾åˆ°çŸ¥è¯†å›¾è°±æ–‡ä»¶")
        except Exception as e:
            error_msg = f"åŠ è½½çŸ¥è¯†å›¾è°±æ—¶å‡ºé”™: {e}"
            logger.error(error_msg)

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        # ä¸è¦ç›´æ¥åœ¨çº¿ç¨‹ä¸­æ“ä½œst.session_stateï¼Œæ”¹ç”¨è¿”å›å€¼æ¨¡å¼
        return {
            "file_manager": file_manager,
            "cache_manager": cache_manager,
            "knowledge_graph_instance": knowledge_graph,
            "vector_store_instance": vector_store,
            "hybrid_retriever_instance": hybrid_retriever,
            "components_loaded": True
        }

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–ç»„ä»¶æ—¶å‡ºé”™: {e}")
        logger.debug(traceback.format_exc())
        return {"components_loaded": False, "error": str(e)}

# åˆå§‹å…¨å±€ç»„ä»¶
global_components = None

# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "user_role" not in st.session_state:
    st.session_state.user_role = None
if "max_contexts" not in st.session_state:
    st.session_state.max_contexts = 3  # è®¾ç½®é»˜è®¤å€¼ä¸º3
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7  # è®¾ç½®é»˜è®¤å€¼
if "max_conversation_history" not in st.session_state:
    st.session_state.max_conversation_history = 5  # è®¾ç½®å¯¹è¯å†å²è®°å½•æœ€å¤§æ¡æ•°
if "include_thinking_in_history" not in st.session_state:
    st.session_state.include_thinking_in_history = False  # æ˜¯å¦åœ¨å¯¹è¯å†å²ä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹
if "use_stream" not in st.session_state:
    st.session_state.use_stream = True  # é»˜è®¤å¯ç”¨æµå¼å“åº”
# LLMé…ç½®
if "llm_type" not in st.session_state:
    st.session_state.llm_type = "ollama"  # é»˜è®¤ä½¿ç”¨ollama
if "ollama_url" not in st.session_state:
    st.session_state.ollama_url = OLLAMA_API_URL
if "ollama_model" not in st.session_state:
    st.session_state.ollama_model = OLLAMA_MODEL
if "custom_base_url" not in st.session_state:
    st.session_state.custom_base_url = ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = ""
if "custom_model" not in st.session_state:
    st.session_state.custom_model = "deepseek-chat"
# ç»„ä»¶åŠ è½½çŠ¶æ€
if "components_loaded" not in st.session_state:
    st.session_state["components_loaded"] = False
# --- End ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---

# åˆå§‹åŒ–ç»„ä»¶ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­è¿è¡Œï¼‰
@st.cache_resource(ttl=3600)
def load_components():
    """åŠ è½½å¹¶ç¼“å­˜ç»„ä»¶ï¼Œç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡"""
    global global_components
    if global_components is None:
        global_components = init_components()
    return global_components

# åº”ç”¨ç»„ä»¶åˆ°ä¼šè¯çŠ¶æ€
def update_session_with_components():
    """å°†ç»„ä»¶æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€"""
    components = load_components()
    if components and components.get("components_loaded", False):
        # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„ç»„ä»¶
        for key, value in components.items():
            st.session_state[key] = value
        return True
    return False

# è·å–Ollamaå¯ç”¨æ¨¡å‹åˆ—è¡¨
def get_ollama_models(api_url: str = "http://localhost:11434") -> List[str]:
    """è·å–OllamaæœåŠ¡å™¨ä¸Šå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        import requests
        response = requests.get(f"{api_url}/api/tags")
        if response.status_code == 200:
            data = response.json()
            models = [model["name"] for model in data.get("models", [])]
            return models
        else:
            logger.warning(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return []

# è®°å½•promptå’Œä¸Šä¸‹æ–‡ä¿¡æ¯
# å…¨å±€æ¶ˆæ¯é›†åˆç”¨äºæ—¥å¿—å»é‡
logged_messages = set()

def log_prompt_context(prompt: str, context: List[str], references: List[str], llm_response: Optional[str] = None):
    """è®°å½•å‘é€ç»™LLMçš„promptå†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶éªŒè¯è¾“å…¥è¾“å‡º
    
    Args:
        prompt: å‘é€ç»™LLMçš„promptå†…å®¹
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ—è¡¨
        references: å‚è€ƒèµ„æ–™åˆ—è¡¨
        llm_response: LLMçš„å“åº”å†…å®¹(å¯é€‰)
    """
    if 1:
        
        def prompt_logger(message: str):
            if message not in logged_messages:
                prompt_logger.info(message)
                logged_messages.add(message)
        
        # è®°å½•æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        prompt_logger.info(f"===== äº¤äº’æ—¶é—´: {timestamp} =====")
        
        # è®°å½•è¾“å…¥éªŒè¯
        prompt_logger.info("===== è¾“å…¥éªŒè¯ =====")
        prompt_logger.info(f"Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        prompt_logger.info(f"ä¸Šä¸‹æ–‡æ•°é‡: {len(context)}")
        prompt_logger.info(f"å‚è€ƒèµ„æ–™æ•°é‡: {len(references)}")
        
        # è®°å½•è¯¦ç»†å†…å®¹
        prompt_logger.info("===== å®é™…Promptå†…å®¹ =====")
        prompt_logger.info(prompt)
        
        prompt_logger.info("===== ä¸Šä¸‹æ–‡ä¿¡æ¯ =====")
        for idx, ctx in enumerate(context, 1):
            prompt_logger.info(f"ä¸Šä¸‹æ–‡ {idx}({len(ctx)}å­—ç¬¦): {ctx[:200]}...")
            
        prompt_logger.info("===== å‚è€ƒèµ„æ–™ =====")
        for idx, ref in enumerate(references, 1):
            prompt_logger.info(f"å‚è€ƒ {idx}({len(ref)}å­—ç¬¦): {ref[:200]}...")
            
        # è®°å½•è¾“å‡ºéªŒè¯(å¦‚æœæœ‰)
        if llm_response:
            prompt_logger.info("===== è¾“å‡ºéªŒè¯ =====")
            prompt_logger.info(f"å“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
            prompt_logger.info(f"å“åº”å†…å®¹: {llm_response[:500]}{'...' if len(llm_response) > 500 else ''}")
            
        prompt_logger.info("=" * 40)

# --- ç”¨æˆ·è®¤è¯ ---
def authenticate(username: str, password: str) -> bool:
    """ç”¨æˆ·è®¤è¯"""
    # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–é…ç½®æ–‡ä»¶ä¸­è·å–ç”¨æˆ·ä¿¡æ¯
    users = {
        "admin": bcrypt.hashpw("admin123".encode("utf-8"), bcrypt.gensalt()),
        "user": bcrypt.hashpw("user123".encode("utf-8"), bcrypt.gensalt()),
    }

    if username in users:
        return bcrypt.checkpw(password.encode("utf-8"), users[username])
    return False

def extract_thinking_process(content: str) -> tuple:
    """ä»å†…å®¹ä¸­æå–æ€è€ƒè¿‡ç¨‹å¹¶æ¸…ç†å›ç­”

    Args:
        content: åŒ…å«æ€è€ƒè¿‡ç¨‹çš„å®Œæ•´å›ç­”å†…å®¹

    Returns:
        tuple: (æ¸…ç†åçš„å†…å®¹, æ€è€ƒè¿‡ç¨‹å†…å®¹)ï¼Œå¦‚æœæ²¡æœ‰æ€è€ƒè¿‡ç¨‹ï¼Œåˆ™æ€è€ƒè¿‡ç¨‹ä¸ºNone
    """
    thought_content = None
    cleaned_content = content

    if "<think>" in content and "</think>" in content:
        try:
            # æå–æ€è€ƒè¿‡ç¨‹å†…å®¹
            start_index = content.find("<think>") + len("<think>")
            end_index = content.find("</think>")
            if start_index > 0 and end_index > start_index:
                thought_content = content[start_index:end_index].strip()

                # ä»ç­”æ¡ˆä¸­å®Œå…¨åˆ é™¤æ€è€ƒè¿‡ç¨‹éƒ¨åˆ†ï¼ŒåŒ…æ‹¬æ ‡ç­¾
                cleaned_content = content[:content.find("<think>")] + content[content.find("</think>") + len("</think>"):].strip()
        except Exception as e:
            logger.error(f"è§£ææ€è€ƒè¿‡ç¨‹æ—¶å‡ºé”™: {e}")

    return cleaned_content, thought_content

# è‡ªå®šä¹‰CSS
def apply_custom_css():
    """åº”ç”¨è‡ªå®šä¹‰CSSæ ·å¼"""
    st.markdown(
        """
    <style>
        /* é¡µé¢æ•´ä½“æ ·å¼ */
        :root {
            --primary-color: #4f6a92;
            --secondary-color: #f5f7fa;
            --text-color: #333333;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        }

        .main {
            background-color: #fff;
            padding: 1rem;
        }

        /* æ ‡é¢˜æ ·å¼ */
        h1 {
            color: var(--text-color);
            font-weight: 600;
            margin-bottom: 1rem;
        }

        /* èŠå¤©æ¶ˆæ¯å®¹å™¨æ ·å¼ */
        [data-testid="stChatMessageContent"] {
            max-width: 90%;
            padding: 0.8rem 1rem;
            border: none !important;
            box-shadow: var(--box-shadow);
            border-radius: var(--border-radius);
            line-height: 1.5;
        }

        /* ç”¨æˆ·æ¶ˆæ¯æ ·å¼ */
        [data-testid="stChatMessage"][data-testid="user"] [data-testid="stChatMessageContent"] {
            background-color: var(--primary-color) !important;
            color: white !important;
        }

        /* åŠ©æ‰‹æ¶ˆæ¯æ ·å¼ */
        [data-testid="stChatMessage"][data-testid="assistant"] [data-testid="stChatMessageContent"] {
            background-color: var(--secondary-color) !important;
            color: var(--text-color) !important;
        }

        /* æ‰©å±•å™¨æ ·å¼ */
        .streamlit-expanderHeader {
            font-weight: 500;
            color: var(--text-color);
            background-color: white;
            border-radius: var(--border-radius);
            padding: 0.7rem 1rem;
            margin-bottom: 0.5rem;
            border: 1px solid #eaeaea;
        }

        /* æ‰©å±•å™¨å†…å®¹æ ·å¼ */
        .streamlit-expanderContent {
            padding: 10px;
            background-color: #fafafa;
            border-radius: 0 0 var(--border-radius) var(--border-radius);
            border: 1px solid #eaeaea;
            border-top: none;
            margin-bottom: 10px;
        }

        /* æ€è€ƒè¿‡ç¨‹å®¹å™¨æ ·å¼ */
        .thinking-container {
            padding: 0.8rem;
            background-color: #f8f9fa;
            border-radius: var(--border-radius);
            border-left: 3px solid var(--primary-color);
        }
    </style>
    """,
        unsafe_allow_html=True,
    )

# åº”ç”¨è‡ªå®šä¹‰CSS
apply_custom_css()

# --- ä¸»ç•Œé¢ ---
st.title("ğŸº æ™ºèƒ½é™¶ç“·é—®ç­”ç³»ç»Ÿ")
st.markdown("##### ä¸€å™¨ä¸€ä¸–ç•Œï¼Œé™¶ç„¶å…±å¤ä»Š")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("å¯¼èˆªä¸è®¾ç½®")

    # æ£€æŸ¥å¹¶åŠ è½½ç»„ä»¶
    components_loaded = update_session_with_components()

    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    if not components_loaded:
        with st.spinner("ç³»ç»Ÿç»„ä»¶æ­£åœ¨åŠ è½½ä¸­..."):
            st.info("ç³»ç»Ÿç»„ä»¶æ­£åœ¨åŠ è½½ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚")

            # æ˜¾ç¤ºåŠ è½½è¿›åº¦å ä½ç¬¦
            progress_placeholder = st.empty()
            # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦åŠ è½½å®Œæˆ
            if "loading_progress" not in st.session_state:
                st.session_state["loading_progress"] = 0

            if st.session_state["loading_progress"] < 100:
                progress_bar = progress_placeholder.progress(st.session_state["loading_progress"])
                st.session_state["loading_progress"] += 20
                if st.session_state["loading_progress"] > 100:
                    st.session_state["loading_progress"] = 100
    else:
        st.success("ç³»ç»Ÿç»„ä»¶å·²åŠ è½½å®Œæˆï¼")

    if st.session_state.user_role is None:
        st.header("ç™»å½•")
        login_username = st.text_input("ç”¨æˆ·å", key="login_user")
        login_password = st.text_input("å¯†ç ", type="password", key="login_pass")
        if st.button("ç™»å½•", key="login_button"):
            if authenticate(login_username, login_password):
                st.session_state.user_role = (
                    "admin" if login_username == "admin" else "user"
                )
                st.rerun()  # é‡æ–°è¿è¡Œä»¥æ›´æ–°ç•Œé¢
            else:
                st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
    else:
        st.success(f"æ¬¢è¿, {st.session_state.user_role}!")
        if st.button("é€€å‡ºç™»å½•"):
            # æ¸…ç†ä¼šè¯çŠ¶æ€
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()  # é‡æ–°è¿è¡Œå›åˆ°ç™»å½•çŠ¶æ€

        # --- ç®¡ç†å‘˜ä¾§è¾¹æ  ---
        if st.session_state.user_role == "admin":
            st.markdown("---")
            st.header("ğŸ“ æ–‡ä»¶ç®¡ç†")
            uploaded_files = st.file_uploader(
                "ä¸Šä¼ æ–‡ä»¶ (PDF/TXT/JSON/MDç­‰)",
                type=["pdf", "txt", "json", "md", "docx", "csv"],
                accept_multiple_files=True,
                key="admin_uploader",  # æ·»åŠ å”¯ä¸€key
            )

            if uploaded_files:
                # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
                if not components_loaded:
                    st.warning("ç³»ç»Ÿç»„ä»¶ä»åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•...")
                else:
                    for file in uploaded_files:
                        # è®¡ç®—æ–‡ä»¶çš„MD5å€¼
                        import hashlib
                        file_content = file.getbuffer()
                        file_md5 = hashlib.md5(file_content).hexdigest()

                        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒMD5çš„æ–‡ä»¶å·²å­˜åœ¨äºç¼“å­˜ä¸­
                        md5_exists = False
                        file_to_replace = None

                        for cache_file in os.listdir(CACHE_DIR):
                            cache_file_path = os.path.join(CACHE_DIR, cache_file)
                            if os.path.isfile(cache_file_path) and not cache_file.startswith("qa_cache"):
                                try:
                                    with open(cache_file_path, "rb") as f:
                                        existing_md5 = hashlib.md5(f.read()).hexdigest()
                                        if existing_md5 == file_md5:
                                            md5_exists = True
                                            break
                                        # å¦‚æœæ–‡ä»¶åç›¸åŒä½†MD5ä¸åŒï¼Œæ ‡è®°ä¸ºéœ€è¦æ›¿æ¢
                                        if cache_file == file.name and existing_md5 != file_md5:
                                            file_to_replace = cache_file_path
                                except Exception as e:
                                    logger.error(f"è®¡ç®—ç¼“å­˜æ–‡ä»¶MD5æ—¶å‡ºé”™: {e}")

                        # æ ¹æ®æ£€æŸ¥ç»“æœå¤„ç†æ–‡ä»¶
                        if md5_exists:
                            st.warning(f"æ–‡ä»¶ {file.name} çš„å†…å®¹å·²å­˜åœ¨äºç¼“å­˜åŒºä¸­ï¼Œè·³è¿‡ä¸Šä¼ ã€‚")
                        else:
                            # å¦‚æœå­˜åœ¨åŒåä½†å†…å®¹ä¸åŒçš„æ–‡ä»¶ï¼Œå…ˆåˆ é™¤å®ƒ
                            if file_to_replace and os.path.exists(file_to_replace):
                                try:
                                    os.remove(file_to_replace)
                                    logger.info(f"å·²åˆ é™¤åŒåæ—§æ–‡ä»¶: {file_to_replace}")
                                except Exception as e:
                                    logger.error(f"åˆ é™¤åŒåæ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                            # ä¿å­˜æ–°æ–‡ä»¶åˆ°ç¼“å­˜
                            cache_file_path = os.path.join(CACHE_DIR, file.name)
                            with open(cache_file_path, "wb") as f:
                                f.write(file_content)
                            st.success(f"æ–‡ä»¶ {file.name} å·²ä¸Šä¼ åˆ°ç¼“å­˜åŒº")

            st.markdown("---")
            st.header("ğŸ”„ çŸ¥è¯†åº“æ„å»º")

            # 1. å¤„ç†ç¼“å­˜åŒºæ–‡ä»¶æŒ‰é’®
            if st.button("å¤„ç†ç¼“å­˜æ–‡ä»¶", key="process_cache"):
                # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
                if not components_loaded:
                    st.warning("ç³»ç»Ÿç»„ä»¶ä»åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•...")
                else:
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                        processed_count = 0
                        error_count = 0
                        # ä¿®æ­£ï¼šæ’é™¤ qa_cache ç›®å½•ä¸‹çš„ cache.json æ–‡ä»¶
                        cache_files = [
                            f
                            for f in os.listdir(CACHE_DIR)
                            if os.path.isfile(os.path.join(CACHE_DIR, f))
                            and not (
                                f == "cache.json"
                                and os.path.dirname(os.path.join(CACHE_DIR, f))
                                == QA_CACHE_DIR
                            )
                            and not f.startswith("qa_cache")  # é¿å…æ„å¤–åŒ¹é…
                        ]

                        if not cache_files:
                            st.info("ç¼“å­˜åŒºæ²¡æœ‰å¾…å¤„ç†çš„æ–‡ä»¶ã€‚")
                        else:
                            progress_bar = st.progress(0)
                            for i, filename in enumerate(cache_files):
                                try:
                                    # ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨å¤„ç†æ–‡ä»¶
                                    result = st.session_state["file_manager"].process_file(filename)
                                    st.success(
                                        f"æ–‡ä»¶ {filename} å·²å¤„ç†å¹¶ä¿å­˜ä¸º {os.path.basename(result['file_path'])}"
                                    )
                                    processed_count += 1
                                except Exception as e:
                                    st.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
                                    error_count += 1
                                progress_bar.progress((i + 1) / len(cache_files))
                            st.info(
                                f"å¤„ç†å®Œæˆï¼šæˆåŠŸ {processed_count} ä¸ª, å¤±è´¥ {error_count} ä¸ªã€‚è¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®é‡å»ºç´¢å¼•ã€‚"
                            )

            # 2. é‡å»ºç´¢å¼•æŒ‰é’®
            if st.button("é‡å»ºçŸ¥è¯†åº“ç´¢å¼•", key="rebuild_index"):
                # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
                if not components_loaded:
                    st.warning("ç³»ç»Ÿç»„ä»¶ä»åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•...")
                else:
                    with st.spinner("æ­£åœ¨è¯»å–æ–‡ä»¶å¹¶æ„å»ºç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±..."):
                        try:
                            from utils.file_handlers.file_manager import FileManager
                            from utils.graph.knowledge_graph import KnowledgeGraph
                            from utils.retrieval.vector_store import VectorStore
                            from utils.retrieval.retriever import HybridRetriever

                            # åˆå§‹åŒ–æ–°çš„å‘é‡å­˜å‚¨å’ŒçŸ¥è¯†å›¾è°±å®ä¾‹
                            st.write("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
                            new_knowledge_graph = KnowledgeGraph()

                            # åˆå§‹åŒ–æ–°çš„å‘é‡å­˜å‚¨
                            st.write("æ­£åœ¨åˆ›å»ºå‘é‡å­˜å‚¨...")
                            new_vector_store = VectorStore(
                                storage_dir=STORAGE_DIR,
                                embedding_model=EMBEDDINGS_MODEL,
                                base_url=OLLAMA_API_URL,
                                debug_mode=DEBUG_MODE,
                                log_file=LOG_FILE
                            )

                            # è¯»å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
                            all_texts = []
                            all_metadatas = []
                            for root, _, files in os.walk(DATA_DIR):
                                for file in files:
                                    if file.endswith((".txt", ".md", ".json")):
                                        try:
                                            file_path = os.path.join(root, file)
                                            with open(file_path, "r", encoding="utf-8") as f:
                                                content = f.read()
                                                all_texts.append(content)
                                                all_metadatas.append({
                                                    "source": file_path,
                                                    "type": os.path.splitext(file)[1][1:],
                                                    "timestamp": datetime.now().isoformat()
                                                })
                                        except Exception as e:
                                            st.error(f"è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}")

                            # åˆ›å»ºå‘é‡å­˜å‚¨
                            if all_texts:
                                new_vector_store.create_from_texts(all_texts, all_metadatas)
                                st.write(f"å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(all_texts)} ä¸ªæ–‡æ¡£ã€‚")
                            else:
                                st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶æ¥åˆ›å»ºå‘é‡å­˜å‚¨ã€‚")

                            # åˆ›å»ºæ–°çš„æ··åˆæ£€ç´¢å™¨
                            new_hybrid_retriever = HybridRetriever(
                                vector_store=new_vector_store,
                                use_reranker=True,
                                rerank_model="BAAI/bge-reranker-base"
                            )

                            # æ„å»ºå’Œä¿å­˜çŸ¥è¯†å›¾è°±
                            new_knowledge_graph.build_from_text("\n\n".join(all_texts))
                            new_knowledge_graph.save_graph(GRAPH_PATH)
                            st.write(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ ({new_knowledge_graph.graph.number_of_nodes()} èŠ‚ç‚¹, {new_knowledge_graph.graph.number_of_edges()} è¾¹) å¹¶å·²ä¿å­˜ã€‚")

                            # ä¿å­˜å‘é‡å­˜å‚¨
                            new_vector_store.save(VECTOR_STORE_FILENAME)
                            st.write(f"å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {VECTOR_STORE_FILENAME}")

                            # åœ¨Streamlitä¼šè¯çŠ¶æ€ä¸­æ›´æ–°å®ä¾‹
                            st.session_state["knowledge_graph_instance"] = new_knowledge_graph
                            st.session_state["hybrid_retriever_instance"] = new_hybrid_retriever
                            st.session_state["vector_store_instance"] = new_vector_store

                            # è°ƒç”¨update_session_with_componentsä»¥åˆ·æ–°ç¼“å­˜
                            st.rerun()

                            st.success("çŸ¥è¯†åº“ç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±å·²æˆåŠŸé‡å»ºï¼")
                        except Exception as e:
                            st.error(f"é‡å»ºç´¢å¼•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                            if DEBUG_MODE:
                                st.code(traceback.format_exc())

            st.header(" çŸ¥è¯†åº“ç®¡ç† (è‡ªå®šä¹‰å†…å®¹)")
            try:
                # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
                if components_loaded and "file_manager" in st.session_state:
                    data_files_list = st.session_state["file_manager"].list_files(CUSTOM_FILES_DIR)
                    if data_files_list:
                        st.dataframe(data_files_list)
                        selected_file_data = st.selectbox(
                            "é€‰æ‹©æ–‡ä»¶è¿›è¡Œæ“ä½œ",
                            [f["name"] for f in data_files_list],
                            key="data_select",
                        )
                        col1_data, col2_data = st.columns(2)
                        with col1_data:
                            if st.button("åˆ é™¤æ–‡ä»¶", key="delete_data"):
                                if st.session_state["file_manager"].delete_file(
                                    selected_file_data, CUSTOM_FILES_DIR
                                ):
                                    st.success("æ–‡ä»¶å·²åˆ é™¤")
                                    st.rerun()
                                else:
                                    st.error("åˆ é™¤æ–‡ä»¶å¤±è´¥")
                        with col2_data:
                            new_name_data = st.text_input(
                                "æ–°æ–‡ä»¶å", key="rename_data_input"
                            )
                            if st.button("é‡å‘½åæ–‡ä»¶", key="rename_data"):
                                if new_name_data and st.session_state["file_manager"].rename_file(
                                    selected_file_data, new_name_data, CUSTOM_FILES_DIR
                                ):
                                    st.success("æ–‡ä»¶å·²é‡å‘½å")
                                    st.rerun()
                                else:
                                    st.error("é‡å‘½åå¤±è´¥æˆ–æœªè¾“å…¥æ–°æ–‡ä»¶å")
                    else:
                        st.info("ç›®å½•ä¸ºç©ºã€‚")
                else:
                    st.warning("ç»„ä»¶æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•...")
            except FileNotFoundError:
                st.info("ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©ºã€‚")
            except Exception as e:
                st.error(f"åŠ è½½ç›®å½•æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}")

            st.markdown("---")
            st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")

            # LLMç±»å‹é€‰æ‹©
            st.session_state.llm_type = st.radio(
                "é€‰æ‹©LLMç±»å‹",
                options=["ollama", "è‡ªå®šä¹‰LLM"],
                horizontal=True,
                key="llm_type_radio",
            )

            # æ ¹æ®é€‰æ‹©çš„LLMç±»å‹æ˜¾ç¤ºä¸åŒçš„é…ç½®é€‰é¡¹
            if st.session_state.llm_type == "ollama":
                st.session_state.ollama_url = st.text_input(
                    "Ollama API URL",
                    value=st.session_state.ollama_url,
                    key="ollama_url_input",
                )

                # è·å–Ollamaå¯ç”¨æ¨¡å‹åˆ—è¡¨
                ollama_models = get_ollama_models(st.session_state.ollama_url)

                if ollama_models:
                    st.session_state.ollama_model = st.selectbox(
                        "Ollama æ¨¡å‹",
                        options=ollama_models,
                        index=(
                            ollama_models.index(st.session_state.ollama_model)
                            if st.session_state.ollama_model in ollama_models
                            else 0
                        ),
                        key="ollama_model_select",
                    )
                else:
                    st.warning("æ— æ³•è·å–Ollamaæ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                
                st.session_state.ollama_model = st.text_input(
                    "Ollama æ¨¡å‹",
                    value=st.session_state.ollama_model,
                    key="ollama_model_input",
                )
                
                # æ·»åŠ æµå¼å“åº”é€‰é¡¹
                if "use_stream" not in st.session_state:
                    st.session_state.use_stream = True
                st.session_state.use_stream = st.checkbox(
                    "å¯ç”¨æµå¼å“åº”",
                    value=st.session_state.use_stream,
                    help="å¯ç”¨æµå¼å“åº”å¯ä»¥å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒ",
                    key="ollama_stream_checkbox",
                )
            else:
                # è‡ªå®šä¹‰LLMé…ç½®
                st.session_state.custom_base_url = st.text_input(
                    "è‡ªå®šä¹‰LLM Base URL (å¦‚ https://api.deepseek.com)",
                    value=st.session_state.custom_base_url,
                    key="custom_base_url_input",
                )
                st.session_state.openai_key = st.text_input(
                    "API Key",
                    value=st.session_state.openai_key,
                    type="password",
                    key="openai_key_input",
                )
                st.session_state.custom_model = st.text_input(
                    "æ¨¡å‹åç§° (å¦‚ deepseek-chat)",
                    value=st.session_state.custom_model,
                    key="custom_model_input",
                )

                # æ·»åŠ æµå¼å“åº”é€‰é¡¹
                if "use_stream" not in st.session_state:
                    st.session_state.use_stream = False
                st.session_state.use_stream = st.checkbox(
                    "å¯ç”¨æµå¼å“åº”",
                    value=st.session_state.use_stream,
                    help="å¯ç”¨æµå¼å“åº”å¯èƒ½ä¼šæé«˜å“åº”é€Ÿåº¦ï¼Œä½†æŸäº›APIå¯èƒ½ä¸æ”¯æŒ",
                    key="use_stream_checkbox",
                )

            # å‚æ•°è°ƒæ•´
            st.session_state.temperature = st.slider(
                "LLM æ¸©åº¦", 0.0, 1.0, st.session_state.temperature, key="temp_slider",
                help="è®¾ç½®LLMçš„æ¸©åº¦ï¼Œè¶Šé«˜è¶Šéšæœºä½†å¯èƒ½æ›´å‡†ç¡®"
            )
            st.session_state.max_contexts = st.slider(
                "æœ€å¤§ä¸Šä¸‹æ–‡æ•°é‡ (æ£€ç´¢)",
                1,
                10,
                st.session_state.max_contexts,
                key="context_slider",
                help="è®¾ç½®æ£€ç´¢æ—¶ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ•°é‡ï¼Œè¶Šé«˜è¶Šç²¾ç¡®ä½†ä¼šæ¶ˆè€—æ›´å¤šèµ„æº"
            )
            st.session_state.max_conversation_history = st.slider(
                "ä¿ç•™å¯¹è¯è½®æ•°",
                1,
                10,
                st.session_state.max_conversation_history,
                key="history_slider",
                help="è®¾ç½®ä¿ç•™å¤šå°‘è½®å¯¹è¯å†å²ï¼Œæ¯è½®åŒ…å«ä¸€ä¸ªç”¨æˆ·é—®é¢˜å’Œä¸€ä¸ªåŠ©æ‰‹å›ç­”"
            )
            
            st.session_state.include_thinking_in_history = st.checkbox(
                "åœ¨å†å²ä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹",
                value=st.session_state.include_thinking_in_history,
                help="å¯ç”¨åï¼Œæœ‰åŠ©äºä¿æŒæ¨ç†è¿è´¯æ€§, ä½†ä¼šå¢åŠ å“åº”æ—¶é—´ï¼Œæ¶ˆè€—æ›´å¤šèµ„æº",
                key="include_thinking_checkbox"
            )

            st.markdown("---")
            st.header("ğŸ—„ï¸ é—®ç­”ç¼“å­˜ç®¡ç†")
            try:
                # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
                if components_loaded and "cache_manager" in st.session_state:
                    cache_stats = st.session_state["cache_manager"].get_stats()
                    st.write(f"ç¼“å­˜å¤§å°: {cache_stats['size']}/{cache_stats['max_size']}")
                    st.write(f"ç¼“å­˜è¿‡æœŸæ—¶é—´ (TTL): {cache_stats['ttl']} ç§’")

                    if st.button("æ¸…ç©ºé—®ç­”ç¼“å­˜", key="clear_qa_cache"):
                        st.session_state["cache_manager"].clear()
                        st.success("é—®ç­”ç¼“å­˜å·²æ¸…ç©º")
                else:
                    st.warning("ç»„ä»¶æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•...")
            except Exception as e:
                st.error(f"è·å–ç¼“å­˜çŠ¶æ€æ—¶å‡ºé”™: {e}")
        # --- End ç®¡ç†å‘˜ä¾§è¾¹æ  ---

# --- æ˜¾ç¤ºä¸»ç•Œé¢å†…å®¹ ---
if st.session_state.user_role is None:
    st.info("ğŸ‘ˆ è¯·åœ¨ä¾§è¾¹æ ç™»å½•ä»¥å¼€å§‹æé—®ã€‚")
else:
    # --- èŠå¤©ç•Œé¢ ---
    # æ·»åŠ æ§åˆ¶æŒ‰é’®ï¼ˆåœ¨ä¾§è¾¹æ ï¼‰
    with st.sidebar:
        st.markdown("---")
        st.header("ğŸ’¬ èŠå¤©è®¾ç½®")

        # ä½¿ç”¨åˆ—å¸ƒå±€è®©æ§åˆ¶æŒ‰é’®æ›´ç¾è§‚
        show_context = st.toggle(
            "æ˜¾ç¤ºæ£€ç´¢ä¸Šä¸‹æ–‡", value=False, key="show_context_toggle", help="æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹"
        )
        show_knowledge_graph = st.toggle(
            "æ˜¾ç¤ºçŸ¥è¯†å›¾è°±", value=False, key="show_kg_toggle", help="æ˜¾ç¤ºç›¸å…³çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯"
        )
        
        # æ·»åŠ æ¸…é™¤å†å²å¯¹è¯æŒ‰é’®
        st.markdown("---")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å†å²å¯¹è¯", key="clear_history_button", type="primary"):
            # æ¸…é™¤æ¶ˆæ¯å†å²
            st.session_state.messages = []
            # æ¸…é™¤é—®ç­”ç¼“å­˜
            if "cache_manager" in st.session_state and st.session_state["cache_manager"]:
                st.session_state["cache_manager"].clear()
            st.success("å†å²å¯¹è¯å·²æ¸…é™¤ï¼")
            st.rerun()

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        if message["role"] == "user":
            with st.chat_message(message["role"], avatar="ğŸ‘¤"):
                st.markdown(message["content"])
        else:
            # æå–åŠ©æ‰‹æ¶ˆæ¯ä¸­çš„æ€è€ƒè¿‡ç¨‹
            content, thought_content = extract_thinking_process(message["content"])
            
            # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯ - ä½¿ç”¨å½“å‰é€‰æ‹©çš„LLMç±»å‹çš„å¤´åƒ
            avatar_icon = "ğŸ¤–"  
            
            # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
            with st.chat_message("assistant", avatar=avatar_icon):
                # é¦–å…ˆæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                if thought_content:
                    with st.expander("ğŸ§  æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(f"<div class='thinking-container'>{thought_content}</div>", unsafe_allow_html=True)
                
                # ç„¶åæ˜¾ç¤ºç­”æ¡ˆ
                st.markdown(content)

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ™¯å¾·é•‡é™¶ç“·ï¼Ÿ"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•å‰ï¼Œç¡®ä¿å†å²è®°å½•ä¸è¶…è¿‡æœ€å¤§é™åˆ¶
        # æ¯ç»„å¯¹è¯åŒ…å«ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”ï¼Œå› æ­¤æ£€æŸ¥å½“å‰æ¶ˆæ¯æ•°é‡æ˜¯å¦è¶…è¿‡ (max_history*2)-1
        max_history_items = st.session_state.max_conversation_history * 2
        if len(st.session_state.messages) >= max_history_items:
            # ç§»é™¤æœ€æ—©çš„ä¸€ç»„å¯¹è¯ï¼ˆä¸€é—®ä¸€ç­”ï¼‰
            st.session_state.messages = st.session_state.messages[2:]

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»„ä»¶å·²åŠ è½½
        components_loaded = update_session_with_components()
        if not components_loaded:
            with st.chat_message("assistant", avatar="ğŸº"):
                st.warning("ç³»ç»Ÿç»„ä»¶æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•...")
            st.session_state.messages.append(
                {"role": "assistant", "content": "ç³»ç»Ÿç»„ä»¶æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•..."}
            )
        else:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = st.session_state["cache_manager"].get(prompt)
            if cached_result:
                with st.chat_message("assistant", avatar="ğŸº"):
                    # æå–å¹¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                    content, thought_content = extract_thinking_process(cached_result["answer"])

                    # é¦–å…ˆæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                    if thought_content:
                        with st.expander("ğŸ§  æ€è€ƒè¿‡ç¨‹", expanded=False):
                            st.markdown(f"<div class='thinking-container'>{thought_content}</div>", unsafe_allow_html=True)

                    # ç„¶åæ˜¾ç¤ºç­”æ¡ˆ
                    st.markdown(content)

                # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²å’Œç¼“å­˜ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨æ ‡ç­¾åŒ…è£…ï¼‰
                full_message = content
                if thought_content:
                    full_message = f"<think>{thought_content}</think>\n\n{content}"
                    
                st.session_state["cache_manager"].set(prompt, {"answer": full_message})
                st.session_state.messages.append(
                    {"role": "assistant", "content": full_message}
                )
            else:
                # ç”Ÿæˆå›ç­”
                with st.spinner("æ€è€ƒä¸­..."):
                    try:
                        import requests
                        # ä½¿ç”¨ä¼šè¯çŠ¶æ€ä¸­çš„æ£€ç´¢å™¨
                        current_retriever = st.session_state["hybrid_retriever_instance"]
                        current_graph = st.session_state["knowledge_graph_instance"]

                        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
                        docs = current_retriever.retrieve(
                            prompt, k=st.session_state.max_contexts
                        )

                        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆå¦‚æœå¼€å¯äº†æ˜¾ç¤ºä¸Šä¸‹æ–‡é€‰é¡¹ï¼‰
                        if show_context and docs:
                            with st.expander("ğŸ“š ç›¸å…³èµ„æ–™", expanded=False):
                                for i, doc in enumerate(docs):
                                    # è·å–æ–‡ä»¶è·¯å¾„ï¼Œæå–æ–‡ä»¶åå¹¶ç§»é™¤æ‰©å±•å
                                    source_path = doc.get('metadata', {}).get('source', 'æœªçŸ¥æ¥æº')
                                    file_name = os.path.basename(source_path)
                                    file_name_without_ext = os.path.splitext(file_name)[0]
                                    st.markdown(
                                        f"**èµ„æ–™ {i+1}** - {file_name_without_ext}"
                                    )
                                    st.markdown(doc.get("content", "å†…å®¹ä¸ºç©º"))

                        # 2. æŸ¥è¯¢çŸ¥è¯†å›¾è°±
                        graph_context = ""
                        graph_results = []
                        if (
                            current_graph
                            and hasattr(current_graph, "graph")
                            and current_graph.graph.number_of_nodes() > 0
                        ):
                            try:
                                # é™åˆ¶åªè¿”å›å‰top_kä¸ªæœ€ç›¸å…³çš„å®ä½“
                                graph_results = current_graph.query(prompt, top_k=20)
                                if graph_results:
                                    graph_context_list = []
                                    for result in graph_results:
                                        # åªå–å‰top_kä¸ªç›¸å…³å®ä½“ï¼Œå‡å°‘tokenä½¿ç”¨
                                        related = ", ".join(
                                            result.get("related_entities", [])[:10]
                                        )
                                        graph_context_list.append(
                                            f"ã€ŒçŸ¥è¯†å›¾è°±ç›¸å…³: {result['entity']} å…³è”è¯: {related}ã€"
                                        )
                                    graph_context = "\n".join(graph_context_list)

                                    # æ˜¾ç¤ºçŸ¥è¯†å›¾è°±ç»“æœï¼ˆå¦‚æœå¼€å¯äº†æ˜¾ç¤ºçŸ¥è¯†å›¾è°±é€‰é¡¹ï¼‰
                                    if show_knowledge_graph and graph_context:
                                        with st.expander(
                                            "ğŸ” çŸ¥è¯†å›¾è°±", expanded=False
                                        ):
                                            for result in graph_results:
                                                st.markdown(
                                                    f"**å®ä½“:** {result['entity']}"
                                                )
                                                if (
                                                    "related_entities" in result
                                                    and result["related_entities"]
                                                ):
                                                    st.markdown(
                                                        f"**å…³è”å®ä½“:** {', '.join(result['related_entities'])}"
                                                    )
                            except Exception as e:
                                st.warning(f"æŸ¥è¯¢çŸ¥è¯†å›¾è°±æ—¶å‡ºé”™: {e}")

                        # 3. æ„å»ºæç¤º
                        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
                        context_parts = []

                        # æ·»åŠ æ£€ç´¢åˆ°çš„æ–‡æ¡£
                        if docs:
                            doc_texts = []
                            for i, doc in enumerate(docs):                                
                                content = doc.get("content", "")
                                # æå–æ–‡ä»¶åå¹¶ç§»é™¤æ‰©å±•å
                                # source = doc.get("metadata", {}).get(
                                #     "source", f"æ¥æº{i+1}"
                                # )
                                # doc_texts.append(f"[{source}]: {content}")

                                doc_texts.append(f"å‚è€ƒèµ„æ–™ {i+1}: {content}")
                            context_parts.append("\n".join(doc_texts))

                        # æ·»åŠ çŸ¥è¯†å›¾è°±ä¿¡æ¯
                        if graph_context:
                            context_parts.append(f"çŸ¥è¯†å›¾è°±ä¿¡æ¯:\n{graph_context}")

                        # åˆå¹¶ä¸Šä¸‹æ–‡ä¿¡æ¯
                        context_str = (
                            "\n\n".join(context_parts)
                            if context_parts
                            else "æ— å¯ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯"
                        )

                        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
                        chat_history = ""
                        if len(st.session_state.messages) > 1:
                            # è·å–æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆä¸åŒ…æ‹¬å½“å‰é—®é¢˜ï¼‰
                            history_messages = st.session_state.messages[:-1]
                            
                            # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡
                            max_pairs = st.session_state.max_conversation_history - 1
                            if len(history_messages) > max_pairs * 2:
                                history_messages = history_messages[-(max_pairs * 2):]
                            
                            # æ ¼å¼åŒ–å¯¹è¯å†å² - ä¼˜åŒ–æ ¼å¼æ›´ç®€æ´æ¸…æ™°
                            formatted_history = []
                            for i in range(0, len(history_messages), 2):
                                if i+1 < len(history_messages):
                                    user_msg = history_messages[i]["content"]
                                    assistant_msg = history_messages[i+1]["content"]
                                    
                                    # æå–æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆå†…å®¹
                                    answer_content, thought_content = extract_thinking_process(assistant_msg)
                                    
                                    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åœ¨å†å²å¯¹è¯ä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹
                                    include_thinking_in_history = st.session_state.include_thinking_in_history
                                    
                                    # ä¼˜åŒ–å†å²æ ¼å¼æ›´ç®€æ´
                                    if include_thinking_in_history and thought_content:
                                        assistant_formatted = f"æ€è€ƒï¼š{thought_content}\nç­”æ¡ˆï¼š{answer_content}"
                                    else:
                                        assistant_formatted = answer_content
                                        
                                    formatted_history.append(f"ç”¨æˆ·ï¼š{user_msg}\nåŠ©æ‰‹ï¼š{assistant_formatted}")
                            
                            chat_history = "\n\n".join(formatted_history)

                        # æ„å»ºç³»ç»Ÿæç¤ºè¯ - ä¼˜åŒ–æç¤ºè¯å‡å°‘å¯¹å‚è€ƒèµ„æ–™çš„è¿‡åº¦å…³æ³¨
                        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨äºé™¶ç“·é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. ä¸“ä¸šæ€§ï¼šç²¾é€šé™¶ç“·çŸ¥è¯†ï¼Œå¯¹é™¶ç“·æ–‡åŒ–æœ‰æ·±åˆ»ç†è§£
2. ä¸Šä¸‹æ–‡ç†è§£ï¼šèƒ½å‡†ç¡®ç†è§£å’Œåˆ©ç”¨å¯¹è¯å†å²ï¼Œä¿æŒå¯¹è¯è¿è´¯æ€§
3. æ€ç»´é€æ˜ï¼šæä¾›æ¸…æ™°çš„æ¨ç†è¿‡ç¨‹ï¼Œè§£é‡Šå¦‚ä½•å¾—å‡ºç»“è®º
4. çŸ¥è¯†æ•´åˆï¼šç»“åˆæ£€ç´¢åˆ°çš„æ–‡æ¡£å’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯æä¾›å…¨é¢ç­”æ¡ˆ
5. å¯¹äºåŸºæœ¬çš„å¸¸è¯†é—®é¢˜ï¼Œèƒ½æä¾›åŸºæœ¬ä¿¡æ¯,å¯¹äºä¸“ä¸šåç§°æˆ–è€…æ¦‚å¿µï¼Œå‚è€ƒèµ„æ–™ç»™å‡ºç­”æ¡ˆ

åœ¨å›ç­”æ—¶è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä¿æŒç­”æ¡ˆç®€æ´ã€æ¸…æ™°ã€ç›´æ¥å›åº”ç”¨æˆ·é—®é¢˜
2. ä¼˜å…ˆä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†ä¸è¦ç”Ÿç¡¬å¤åˆ¶æˆ–é€æ¡æ€»ç»“å‚è€ƒèµ„æ–™
3. é¿å…è¿‡åº¦è§£é‡Šå‚è€ƒèµ„æ–™æ¥æºï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¯¢é—®ä¿¡æ¯æ¥æº
4. ä¿æŒè‡ªç„¶çš„å¯¹è¯é£æ ¼ï¼Œä¸è¦è¿‡åº¦å­¦æœ¯åŒ–
5. ä¸è¦èƒ¡ç¼–ä¹±é€ èµ„æ–™,ç¡®ä¿å›ç­”æœ‰æ®å¯ä¾ï¼Œä½†ä»¥ç”¨æˆ·é—®é¢˜ä¸ºä¸­å¿ƒ
6. è‹¥ç”¨æˆ·è¡¨è¾¾éé™¶ç“·ç›¸å…³é—®é¢˜ï¼Œå¯ç¤¼è²Œè‡´è°¢å¹¶å¼•å¯¼è‡³é™¶ç“·è¯é¢˜, å¹¶å¼•å¯¼ç”¨æˆ·ç»§ç»­æé—®"""

                        # æ„å»ºç”¨æˆ·æç¤ºè¯ - ä¼˜åŒ–æŒ‡ä»¤æ›´åŠ æ˜ç¡®
                        user_prompt = f"""è¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼Œç®€æ´æ˜äº†åœ°ç›´æ¥è§£ç­”ç”¨æˆ·çš„ç–‘é—®ï¼š

__å½“å‰é—®é¢˜ï¼š{prompt}__

ä½ å¯ä»¥å‚è€ƒä»¥ä¸‹ä¿¡æ¯,ä½†æ˜¯ä¸è¦å°†è¿™äº›æ¶ˆæ¯è¯¯è®¤ä¸ºé—®é¢˜ï¼Œè¿™äº›åªæ˜¯èµ„æ–™ï¼š

ã€å†å²å¯¹è¯ã€‘
{chat_history if chat_history else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹"}

ã€å‚è€ƒèµ„æ–™ã€‘
{context_str}

è¯·æ³¨æ„ï¼š
- å¦‚æœå†å²å¯¹è¯å’Œå‚è€ƒèµ„æ–™éƒ½æ²¡æœ‰æä¾›ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”"æˆ‘ä¸ç¡®å®š"æˆ–"æˆ‘æ— æ³•å›ç­”"ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆï¼Œå¼•å¯¼ç”¨æˆ·ç»§ç»­æé—®è·å¾—æ›´å¤šä¿¡æ¯
- åªä½¿ç”¨ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œæ— å…³å†…å®¹å¯å¿½ç•¥
"""

                        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                        # print(messages)
                        # 4. è°ƒç”¨LLM - ç»Ÿä¸€å¤„ç†å‡½æ•°
                        # åˆ›å»ºæµå¼å“åº”
                        full_response = ""

                        try:
                            # åˆ›å»ºæ¶ˆæ¯å®¹å™¨ - æ ¹æ®LLMç±»å‹é€‰æ‹©ä¸åŒçš„å¤´åƒ
                            avatar_icon = "ğŸº" 
                            with st.chat_message("assistant", avatar=avatar_icon):
                                message_placeholder = st.empty()

                                # æ ¹æ®é€‰æ‹©çš„LLMç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                                if st.session_state.llm_type == "ollama":
                                    # ä½¿ç”¨Ollama API
                                    try:
                                        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æµå¼å“åº”
                                        use_stream = st.session_state.use_stream if "use_stream" in st.session_state else True
                                        
                                        # è®°å½•å‘é€ç»™Ollamaçš„promptå†…å®¹
                                        logger.info(f"å‘é€ç»™Ollamaçš„promptå†…å®¹:\n{system_prompt}")
                                        logger.info(f"Ollamaè¯·æ±‚å‚æ•°: model={st.session_state.ollama_model}, temperature={st.session_state.temperature}")
                                        
                                        # æ„å»ºå®Œæ•´æç¤ºï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºã€ç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡
                                        complete_prompt = f"{system_prompt}\n\n{user_prompt}"
                                        
                                        # æ›´è¯¦ç»†çš„æ—¥å¿—è®°å½•
                                        if DEBUG_MODE:
                                            logger.debug(f"å‘é€ç»™Ollamaçš„å®Œæ•´æç¤º:\n{complete_prompt}")
                                            # è®°å½•è¾“å…¥åˆ°Promptæ—¥å¿—
                                            log_prompt_context(
                                                complete_prompt, 
                                                [context_str] if context_str else [], 
                                                [], 
                                                None
                                            )
                                        
                                        response = requests.post(
                                            f"{st.session_state.ollama_url}/api/generate",
                                            json={
                                                "model": st.session_state.ollama_model,
                                                "prompt": complete_prompt,  # ä½¿ç”¨å®Œæ•´æç¤º
                                                "stream": use_stream,
                                                "options": {
                                                    "temperature": st.session_state.temperature,
                                                    "num_ctx": 8192
                                                }
                                            },
                                            stream=use_stream,
                                            timeout=60
                                        )
                                        response.raise_for_status()

                                        if use_stream:
                                            # å¤„ç†æµå¼å“åº”
                                            for line in response.iter_lines():
                                                if line:
                                                    data = json.loads(line.decode())
                                                    token = data.get("response", "")
                                                    full_response += token
                                                    message_placeholder.markdown(full_response + "â–Œ")
                                                    if data.get("done", False):
                                                        logger.info(f"Ollamaæµå¼å“åº”å®Œæˆ: {full_response}")
                                                        break
                                        else:
                                            # å¤„ç†éæµå¼å“åº”
                                            response_data = response.json()
                                            full_response = response_data.get("response", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œä»¥åŠæ¨¡å‹æ˜¯å¦æ­£ç¡®ã€‚")
                                            logger.info(f"Ollamaå®Œæ•´å“åº”: {full_response}")
                                    except requests.exceptions.RequestException as e:
                                        error_msg = "è¿æ¥OllamaæœåŠ¡å™¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ"
                                        logger.error(f"Ollama APIé”™è¯¯: {str(e)}")
                                        raise ValueError(error_msg)

                                else:  # ä½¿ç”¨è‡ªå®šä¹‰LLM (OpenAIå…¼å®¹æ¥å£)
                                    # æ£€æŸ¥å¿…è¦çš„é…ç½®
                                    if not st.session_state.custom_base_url or not st.session_state.openai_key:
                                        raise ValueError("è¯·åœ¨ç³»ç»Ÿé…ç½®ä¸­è®¾ç½®å®Œæ•´çš„è‡ªå®šä¹‰LLMä¿¡æ¯ï¼ˆBase URLå’ŒAPI Keyï¼‰")

                                    try:
                                        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
                                        base_url = st.session_state.custom_base_url.strip()
                                        
                                        # ç§»é™¤å¯èƒ½é‡å¤çš„åè®®å‰ç¼€
                                        base_url = base_url.replace("https://https://", "https://")
                                        base_url = base_url.replace("http://http://", "http://")
                                        
                                        # å¦‚æœæ²¡æœ‰åè®®å‰ç¼€ï¼Œæ·»åŠ https://
                                        if not base_url.startswith(('http://', 'https://')):
                                            base_url = f"https://{base_url}"
                                        
                                        # ç§»é™¤URLæœ«å°¾çš„æ–œæ 
                                        base_url = base_url.rstrip('/')
                                        
                                        logger.info(f"æ­£åœ¨è¿æ¥è‡ªå®šä¹‰LLM API: {base_url}")
                                        
                                        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                                        client = OpenAI(
                                            api_key=st.session_state.openai_key,
                                            base_url=base_url,
                                            timeout=60.0,
                                            max_retries=2  # æ·»åŠ é‡è¯•æ¬¡æ•°
                                        )

                                        # æµ‹è¯•APIè¿æ¥
                                        try:
                                            # å°è¯•ä¸€ä¸ªè½»é‡çº§çš„APIè°ƒç”¨æ¥æµ‹è¯•è¿æ¥
                                            test_response = client.models.list()
                                            logger.info("APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                                        except Exception as test_e:
                                            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {str(test_e)}")
                                            if "auth" in str(test_e).lower():
                                                raise ValueError("APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
                                            elif any(err in str(test_e).lower() for err in ["connection", "timeout", "connect"]):
                                                raise ValueError(f"æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {base_url}ï¼Œè¯·æ£€æŸ¥:\n1. æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿")
                                            else:
                                                raise ValueError(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {str(test_e)}")

                                        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                                        messages = [
                                            {"role": "system", "content": system_prompt},
                                            {"role": "user", "content": user_prompt}
                                        ]

                                        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æµå¼å“åº”
                                        use_stream = st.session_state.use_stream if "use_stream" in st.session_state else False

                                        if use_stream:
                                            # æµå¼å“åº”å¤„ç†
                                            stream = client.chat.completions.create(
                                                model=st.session_state.custom_model,
                                                messages=messages,
                                                temperature=st.session_state.temperature,
                                                stream=True
                                            )

                                            # ç”¨äºç´¯ç§¯æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
                                            reasoning_content = ""
                                            final_content = ""

                                            for chunk in stream:
                                                if hasattr(chunk.choices[0], 'delta'):
                                                    delta = chunk.choices[0].delta
                                                    # æ£€æŸ¥æ˜¯å¦æœ‰reasoning_content
                                                    if hasattr(delta, 'reasoning_content'):
                                                        reasoning = delta.reasoning_content
                                                        if reasoning:
                                                            reasoning_content += reasoning
                                                    # æ£€æŸ¥æ™®é€šcontent
                                                    if hasattr(delta, 'content'):
                                                        content = delta.content
                                                        if content:
                                                            final_content += content
                                                            full_response = f"<think>{reasoning_content}</think>\n\n{final_content}"
                                                            message_placeholder.markdown(full_response + "â–Œ")

                                            # ç¡®ä¿æœ€ç»ˆå“åº”åŒ…å«æ€è€ƒè¿‡ç¨‹
                                            if reasoning_content:
                                                full_response = f"<think>{reasoning_content}</think>\n\n{final_content}"
                                            else:
                                                full_response = final_content
                                        else:
                                            # éæµå¼å“åº”
                                            response = client.chat.completions.create(
                                                model=st.session_state.custom_model,
                                                messages=messages,
                                                temperature=st.session_state.temperature,
                                                stream=False
                                            )

                                            # æå–æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
                                            if hasattr(response.choices[0].message, 'reasoning_content'):
                                                reasoning = response.choices[0].message.reasoning_content
                                                content = response.choices[0].message.content
                                                full_response = f"<think>{reasoning}</think>\n\n{content}"
                                            else:
                                                # å¦‚æœæ²¡æœ‰reasoning_contentï¼Œå°±åªä½¿ç”¨æ™®é€šcontent
                                                full_response = response.choices[0].message.content

                                    except Exception as api_error:
                                        error_msg = str(api_error)
                                        logger.error(f"è‡ªå®šä¹‰LLM APIé”™è¯¯: {error_msg}")
                                        
                                        if isinstance(api_error, ValueError):
                                            # ç›´æ¥ä¼ é€’ValueErrorçš„é”™è¯¯ä¿¡æ¯
                                            raise ValueError(error_msg)
                                        elif "auth" in error_msg.lower() or "key" in error_msg.lower() or "token" in error_msg.lower():
                                            raise ValueError("APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
                                        elif any(err in error_msg.lower() for err in ["connection", "timeout", "connect"]):
                                            raise ValueError(f"è¿æ¥æœåŠ¡å™¨å¤±è´¥ ({base_url})ï¼Œè¯·æ£€æŸ¥:\n1. æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿")
                                        else:
                                            raise ValueError(f"è°ƒç”¨APIæ—¶å‘ç”Ÿé”™è¯¯: {error_msg}")

                                # æ¸…ç©ºæµå¼å“åº”å ä½ç¬¦
                                message_placeholder.empty()

                                # è§£ææ€è€ƒè¿‡ç¨‹å¹¶æ¸…ç†ç­”æ¡ˆ
                                answer, thought_content = extract_thinking_process(full_response)

                                # é¦–å…ˆæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                                if thought_content:
                                    with st.expander("ğŸ§  æ€è€ƒè¿‡ç¨‹", expanded=False):
                                        st.markdown(f"<div class='thinking-container'>{thought_content}</div>", unsafe_allow_html=True)

                                # ç„¶ååœ¨æ¶ˆæ¯å®¹å™¨å†…æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆï¼ˆä¸å«æ€è€ƒè¿‡ç¨‹åŠæ ‡ç­¾ï¼‰
                                st.markdown(answer)

                            # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²å’Œç¼“å­˜ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨æ ‡ç­¾åŒ…è£…ï¼‰
                            full_message = answer
                            if thought_content:
                                full_message = f"<think>{thought_content}</think>\n\n{answer}"
                                
                            st.session_state["cache_manager"].set(prompt, {"answer": full_message})
                            st.session_state.messages.append(
                                {"role": "assistant", "content": full_message}
                            )

                        except ValueError as ve:
                            # å¤„ç†å‚æ•°é”™è¯¯
                            with st.chat_message("assistant", avatar="ğŸº"):
                                st.error(f"å‚æ•°é”™è¯¯: {str(ve)}")
                            st.session_state.messages.append(
                                {"role": "assistant", "content": f"é…ç½®é”™è¯¯: {ve}"}
                            )
                        except Exception as e:
                            # å¤„ç†å…¶ä»–é”™è¯¯
                            with st.chat_message("assistant", avatar="ğŸº"):
                                error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                                if "auth" in str(e).lower() or "key" in str(e).lower() or "token" in str(e).lower():
                                    error_msg = "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®"
                                elif "connect" in str(e).lower() or "timeout" in str(e).lower():
                                    error_msg = "è¿æ¥æœåŠ¡å™¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡å™¨åœ°å€"
                                st.error(error_msg)
                            st.session_state.messages.append(
                                {"role": "assistant", "content": f"å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}"}
                            )
                    except Exception as e:
                        # è¿™é‡Œæ•è·ä¸Šå±‚å…¶ä»–å¯èƒ½çš„é”™è¯¯ï¼Œæ¯”å¦‚æ£€ç´¢å™¨é”™è¯¯ç­‰
                        with st.chat_message("assistant", avatar="ğŸº"):
                            st.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                        st.session_state.messages.append(
                            {"role": "assistant", "content": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}"}
                        )
    # --- End èŠå¤©ç•Œé¢ ---
